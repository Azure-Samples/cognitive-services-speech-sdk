Inhalte wie Daten, Modelle, Tests und Endpunkte sind im Custom Speech-Portal in Projekten organisiert. Jedes Projekt ist für eine Domäne und das Land/die Sprache spezifisch. Folgen Sie den Anweisungen des Assistenten zum Erstellen Ihres Projekts. Nachdem Sie ein Projekt erstellt haben, sollten vier Registerkarten angezeigt werden: Daten, Test, Training und Bereitstellung.
Über das Custom Speech-Portal können Sie hochgeladene Audiodaten wiedergeben und bestimmen, ob das angegebene Erkennungsergebnis korrekt ist. Mit diesem Tool können Sie schnell die Qualität des Baseline-Spracherkennungsmodells von Microsoft oder eines trainierten benutzerdefinierten Modells überprüfen, ohne Audiodaten transkribieren zu müssen.
Sobald der Test abgeschlossen ist, was durch den Statuswechsel zu Erfolgreich angezeigt wird, erhalten Sie eine WER-Nummer für beide Modelle in Ihrem Test. 
In diesem Dokument erfahren Sie, wie Sie die Qualität des Spracherkennungsmodells von Microsoft oder Ihres benutzerdefinierten Modells quantitativ messen können. Zum Testen der Genauigkeit sind Audio- und menschenmarkierte Transkriptionsdaten erforderlich, und 30 Minuten bis 5 Stunden an repräsentativem Audio müssen bereitgestellt werden.
Was ist die Wort-Fehler-Rate (Word Error Rate, WER)?
Der Branchenstandard zur Messung der Modellgenauigkeit ist die Wort-Fehler-Rate (WER). WER ermittelt die Anzahl der bei der Erkennung falsch identifizierten Wörter und dividiert sie durch die Gesamtzahl der Wörter, die im menschenmarkierten Transkript angegeben sind. Diese Zahl wird dann mit 100 % multipliziert, um die WER zu berechnen.
WER-Formel
Falsch identifizierte Wörter fallen in drei Kategorien:
Einfügung (I): Wörter, die fälschlicherweise im Hypothesentranskript hinzugefügt werden
Löschung (D): Wörter, die im Hypothesentranskript nicht erkannt werden
Ersetzung (S): Wörter, die zwischen Verweis und Hypothese ersetzt wurden
Hier sehen Sie ein Beispiel:
Beispiel für falsch identifizierte Wörter
Beheben von Fehlern und Verbessern der WER
Sie können die WER aus den Ergebnissen der maschinellen Erkennung verwenden, um die Qualität des mit Ihrer App, Ihrem Tool oder Ihrem Produkt verwendeten Modells zu bewerten. Ein WER von 5 %–10 % gilt als gute Qualität und ist einsatzbereit. Eine WER von 20 % ist akzeptabel, Sie sollten aber weiteres Training in Betracht ziehen. Eine WER von 30 % oder mehr weist auf schlechte Qualität hin und erfordert Anpassung und Training.
Die Verteilung der Fehler spielt eine wichtige Rolle. Wenn viele Löschfehler auftreten, liegt es in der Regel an einem schwachen Audiosignal. Um dieses Problem zu beheben, müssen Sie Audiodaten näher an der Quelle erfassen. Einfügefehler bedeuten, dass der Ton in einer lauten Umgebung aufgenommen wurde und Übersprechen vorhanden sein kann, was zu Erkennungsproblemen führt. Ersetzungsfehler treten häufig auf, wenn eine unzureichende Stichprobe domänenspezifischer Begriffe entweder als menschenmarkierte Transkriptionen oder zugehöriger Text bereitgestellt wurde.
Durch die Analyse einzelner Dateien können Sie feststellen, welche Art von Fehlern vorliegen und welche Fehler nur in einer bestimmten Datei auftreten. Das Verständnis von Problemen auf Dateiebene hilft Ihnen bei gezielten Verbesserungen.
Erstellen eines Tests
Wenn Sie die Qualität des Basismodells für Spracherkennung von Microsoft oder eines von Ihnen trainierten benutzerdefinierten Modells testen möchten, können Sie zwei Modelle nebeneinander vergleichen, um die Genauigkeit zu bewerten. Der Vergleich umfasst WER und Erkennungsergebnisse. In der Regel wird ein benutzerdefiniertes Modell mit dem Basismodell von Microsoft verglichen.
Parallele Auswertung von Modellen:
Melden Sie sich beim Custom Speech-Portal an.
Navigieren Sie zu Spracherkennung > Custom Speech > Testen.
Klicken Sie auf Test hinzufügen.
Wählen Sie Bewerten der Genauigkeit aus. Geben Sie einen Namen und eine Beschreibung für den Test ein, und wählen Sie Ihr Audio- und menschenmarkiertes Transkriptionsdataset aus.
Wählen Sie bis zu zwei Modelle aus, die Sie testen möchten.
Klicken Sie auf Create.
Nachdem der Test erfolgreich erstellt wurde, können Sie die Ergebnisse nebeneinander vergleichen.