# Examples to synthesis with input text stream

The input text stream API is designed to generate audio from text that is being streamed or generated in chunks. A typical scenario is to speak text generated from GPT-like models. Compared to non-text stream APIs, the text stream API significantly reduces TTS latency.

|  | Non text stream | Text Stream |
| ---------- | -------- | ----------- |
| Input Type | Whole GPT response | Each GPT output chunk |
| Latency | High: Time of full GPT response + Time of TTS | Low: Time of few GPT chunks + Time of TTS |

### Available samples:

| Language | Directory | Description |
| ---------- | -------- | ----------- |
| Python | [python](text_stream_sample.py) | synthesis with text stream API, the text stream generated by AOAI GPT chat model  |

## Setup Instructions

### 1. Install Python Dependencies

First, create and activate a virtual environment (recommended):

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

Install required packages:

```bash
pip install -r requirements.txt
```

### 2. Configure Environment Variables

Set the following environment variables with your Azure credentials:

**Required for Azure OpenAI:**
- `AZURE_OPENAI_API_ENDPOINT` - Your Azure OpenAI endpoint (e.g., `https://your-resource.openai.azure.com/`)
- `AZURE_OPENAI_API_KEY` - Your Azure OpenAI API key
- `AZURE_OPENAI_API_VERSION` - API version (e.g., `2024-02-01`)
- `AZURE_OPENAI_DEPLOYMENT_NAME` - Your GPT deployment name

**Required for Azure TTS:**
- `AZURE_TTS_REGION` - Your Azure Speech service region (e.g., `eastus`)
- `AZURE_TTS_API_KEY` - Your Azure Speech service subscription key

**Windows (Command Prompt):**
```cmd
set AZURE_OPENAI_API_ENDPOINT=https://your-resource.openai.azure.com/
set AZURE_OPENAI_API_KEY=your-openai-key
set AZURE_OPENAI_API_VERSION=2024-02-01
set AZURE_OPENAI_DEPLOYMENT_NAME=your-gpt-deployment
set AZURE_TTS_REGION=eastus
set AZURE_TTS_API_KEY=your-tts-key
```

**Windows (PowerShell):**
```powershell
$env:AZURE_OPENAI_API_ENDPOINT="https://your-resource.openai.azure.com/"
$env:AZURE_OPENAI_API_KEY="your-openai-key"
$env:AZURE_OPENAI_API_VERSION="2024-02-01"
$env:AZURE_OPENAI_DEPLOYMENT_NAME="your-gpt-deployment"
$env:AZURE_TTS_REGION="eastus"
$env:AZURE_TTS_API_KEY="your-tts-key"
```

**Linux/Mac:**
```bash
export AZURE_OPENAI_API_ENDPOINT="https://your-resource.openai.azure.com/"
export AZURE_OPENAI_API_KEY="your-openai-key"
export AZURE_OPENAI_API_VERSION="2024-02-01"
export AZURE_OPENAI_DEPLOYMENT_NAME="your-gpt-deployment"
export AZURE_TTS_REGION="eastus"
export AZURE_TTS_API_KEY="your-tts-key"
```

### 3. Run the Sample

**Basic usage (with default settings):**
```bash
python text_stream_sample.py
```

**Advanced usage with command-line options:**
```bash
# Use a specific voice
python text_stream_sample.py --voice "en-US-AriaNeural"

# Customize the GPT prompt
python text_stream_sample.py --prompt "Tell me a story about AI"

# Specify output file
python text_stream_sample.py --output "my_audio.wav"

# Use a custom voice with deployment ID
python text_stream_sample.py --voice "YourCustomVoice" --deployment "your-deployment-id"

# Override region and key (instead of using env variables)
python text_stream_sample.py --region "westus" --key "your-api-key"

# Combine multiple options
python text_stream_sample.py --voice "en-US-JennyNeural" --prompt "Explain quantum computing" --output "quantum.wav"
```

**Available command-line options:**
- `--voice` - Voice name for synthesis (default: `en-us-Ava:DragonHDLatestNeural`)
- `--region` - Azure TTS region (default: from `AZURE_TTS_REGION` env variable)
- `--key` - Azure TTS API key (default: from `AZURE_TTS_API_KEY` env variable)
- `--deployment` - Custom deployment ID for custom voice
- `--output` - Output WAV file path (default: `output_audio.wav`)
- `--prompt` - Prompt for GPT (default: `tell me a joke in 100 words`)

### Features

The sample demonstrates:
- Real-time text-to-speech synthesis from GPT streaming output
- Audio playback through system sound card
- Saving synthesized audio to a WAV file with proper headers
- Support for custom neural voices and deployments
- Low-latency streaming using WebSocket v2 endpoint

## API overview
### Create text stream request
To use the text stream API, you have to use the websocket V2 endpoint.  
```wss://{region}.tts.speech.microsoft.com/cognitiveservices/websocket/v2```

### Set global properties
Since the input of text stream API is parital text. SSML, which is based on XML, is not supported. And thus properties that set in SSML should be set in a new way.  

For now we only support set voice name and output format.

### Create input text stream
Please specify `speechsdk.SpeechSynthesisRequestInputType.TextStream` when creating the request.

### Send text to stream
For each text that generated from GPT, call `request.input_stream.write(text)` to send text to the stream.  
Please do not wait too long(longer than 30s) between creating the request and sending the first text, and also between sending two texts, otherwise the connection may be closed by server and you may got 503 error.

### Close text stream
When GPT finished the output, call `request.input_stream.close()` to close the stream.

